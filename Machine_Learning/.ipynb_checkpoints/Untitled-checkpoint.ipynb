{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3774d12-3bea-4535-ad0c-b0590ee7927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import visualkeras\n",
    "from collections import defaultdict\n",
    "from PIL import ImageFont\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#########################################################\n",
    "# Importing the Dataset\n",
    "#########################################################\n",
    "\n",
    "DATASET_PATH = 'AudioSamples'\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "\n",
    "classes = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "classes = classes[classes != 'dataset.csv']\n",
    "print('Classes: ', classes)\n",
    "\n",
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Example file tensor:', filenames[0])\n",
    "\n",
    "# 70:15:15 split for train, test and validation (808 samples in total)\n",
    "\n",
    "train_files = filenames[:565]\n",
    "val_files = filenames[565: 565 + 121]\n",
    "test_files = filenames[-121:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))\n",
    "\n",
    "#########################################################\n",
    "# Read the Audio files and their labels\n",
    "#########################################################\n",
    "\n",
    "test_file = tf.io.read_file(DATASET_PATH+'/Healthy/4/4_Healthy_High.wav')\n",
    "test_audio, _ = tf.audio.decode_wav(contents=test_file)\n",
    "print(test_audio.shape)\n",
    "\n",
    "def decode_audio(audio_binary):\n",
    "  # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
    "  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
    "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
    "  # Since all the data is single channel (mono), drop the `channels`\n",
    "  # axis from the array.\n",
    "  return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(\n",
    "      input=file_path,\n",
    "      sep=os.path.sep)\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
    "  # to work in a TensorFlow graph.\n",
    "  return parts[-3]\n",
    "\n",
    "def get_waveform_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, label\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "\n",
    "waveform_ds = files_ds.map(\n",
    "    map_func=get_waveform_and_label,\n",
    "    num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "\n",
    "# Plotting waveforms onto a graph \n",
    "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  ax.plot(audio.numpy())\n",
    "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  ax.set_title(label)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# Convert Waveforms to Spectrograms\n",
    "#########################################################\n",
    "\n",
    "# Getting the average shape of 100 waveform samples to use for padding\n",
    "shapeTotal = 0\n",
    "for j, (audio, label) in enumerate(waveform_ds.take(100)):\n",
    "    tensor_shape_tuple = audio.get_shape()\n",
    "    shapeTotal = shapeTotal + tensor_shape_tuple[0]\n",
    "avgShape = round(shapeTotal/100)\n",
    "print(\"Average Shape of Audio: \" + str(avgShape))\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "  # Zero-padding for an audio waveform with less than the average shape of samples.\n",
    "  input_len = avgShape\n",
    "  waveform = waveform[:input_len]\n",
    "  zero_padding = tf.zeros(\n",
    "      [avgShape] - tf.shape(waveform),\n",
    "      dtype=tf.float32)\n",
    "  # Cast the waveform tensors' dtype to float32.\n",
    "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "  # Concatenate the waveform with `zero_padding`, which ensures all audio clips are of the same length.\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram\n",
    "\n",
    "for waveform, label in waveform_ds.take(1):\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "\n",
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, avgShape])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show()\n",
    "\n",
    "def get_spectrogram_and_label_id(audio, label):\n",
    "  spectrogram = get_spectrogram(audio)\n",
    "  label_id = tf.argmax(label == classes)\n",
    "  return spectrogram, label_id\n",
    "\n",
    "spectrogram_ds = waveform_ds.map(\n",
    "  map_func=get_spectrogram_and_label_id,\n",
    "  num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "\n",
    "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  plot_spectrogram(spectrogram.numpy(), ax)\n",
    "  ax.set_title(classes[label_id.numpy()])\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#########################################################\n",
    "# Build and Train CNN Model\n",
    "#########################################################\n",
    "\n",
    "def preprocess_dataset(files):\n",
    "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "  output_ds = files_ds.map(\n",
    "      map_func=get_waveform_and_label,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  output_ds = output_ds.map(\n",
    "      map_func=get_spectrogram_and_label_id,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  return output_ds\n",
    "\n",
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)\n",
    "\n",
    "# batch size = 128\n",
    "train_ds = train_ds.batch(565)\n",
    "val_ds = val_ds.batch(121)\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "  input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(classes)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrogram with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "# Defining layers within Neural Network\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='sigmoid'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "EPOCHS = 100\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    ")\n",
    "\n",
    "font = ImageFont.truetype(\"arial.ttf\", 32)\n",
    "color_map = defaultdict(dict)\n",
    "color_map[layers.Conv2D]['fill'] = 'orange'\n",
    "color_map[layers.ZeroPadding2D]['fill'] = 'gray'\n",
    "color_map[layers.Dropout]['fill'] = 'pink'\n",
    "color_map[layers.MaxPooling2D]['fill'] = 'red'\n",
    "color_map[layers.Dense]['fill'] = 'green'\n",
    "color_map[layers.Flatten]['fill'] = 'teal'\n",
    "\n",
    "# visualkeras.layered_view(model, legend=True, color_map=color_map, to_file='CNNArchitecture.png').show()\n",
    "\n",
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n",
    "#########################################################\n",
    "# Evaluating the Model Performance\n",
    "#########################################################\n",
    "\n",
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "#Making a prediction against generated model using test samples\n",
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Groundtruth')\n",
    "plt.show()\n",
    "\n",
    "#Measuring Type I and Type II errors\n",
    "truePositives=confusion_mtx[0][0].numpy()\n",
    "falseNegatives=confusion_mtx[0][1].numpy()\n",
    "falsePositives=confusion_mtx[1][0].numpy()\n",
    "trueNegatives=confusion_mtx[1][1].numpy()\n",
    "\n",
    "errorRate=(falseNegatives+falsePositives)/len(y_true)\n",
    "recall=truePositives/(truePositives+falseNegatives)\n",
    "precision=truePositives/(truePositives+falsePositives)\n",
    "specificity=trueNegatives/(trueNegatives+falsePositives)\n",
    "fMeasure=(2 * precision * recall)/(precision + recall)\n",
    "falseAlarm=1-specificity\n",
    "\n",
    "print(f'Test Set Accuracy: {test_acc:.0%}')\n",
    "print(f'Test Set Error Rate: {errorRate:.0%}')\n",
    "print(f'Test Set Recall: {recall:.0%}')\n",
    "print(f'Test Set Precision: {precision:.0%}')\n",
    "print(f'Test Set Specificity: {specificity:.0%}')\n",
    "print(f'Test Set F-Measure (F1): {fMeasure:.0%}')\n",
    "print(f'Test Set False Alarm Rate: {falseAlarm:.0%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
